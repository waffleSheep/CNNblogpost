{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f86104a",
   "metadata": {},
   "source": [
    "# Building Convolutional Neural Networks from scratch by Karimi Zayan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a42923b",
   "metadata": {},
   "source": [
    "## Part 1: Matrix calculus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6b7ea8",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231745d7",
   "metadata": {},
   "source": [
    "**Mathematics** is an area of knowledge, which includes the study of such topics as numbers, formulas and related structures, shapes and spaces in which they are contained, and quantities and their changes. There is no general consensus about its exact scope or epistemological status. However, it is extremely cringe and garbage but necessary and is sometimes (very rarely) interesting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533b0ff6",
   "metadata": {},
   "source": [
    "Neural Networks are somewhat interesting. Everyone knows the math behind NNs (the gist of it). It was thought in **CS5131** to a very limited extent and is incredibly matrixy (in its full form) but not many know the math behind Convolutional neural networks. I mean people get that it has something to do with backpropogation or whatever, but how do you find derivative of the convolution function or the derivative of the max pooling function. As you will come to learn, this is incredibly cringe and was a huge time sink. (ESPECIALLY DURING IMPLEMENTATION) But I have done it because I care about appventure or whatever even though like 4 people will read this. (Hello Jun Rong, Prannaya, Jed, Vishal and the other exco members (I forgot your names, Steve or something)) It was a fun but painful exercise (actually I don't know whether this will be fun because I wrote this before doing the work) and I gained a deeper understanding of math??!?! or whatever. Anyway, you are here so uh, we are gonna start out with a refresher **(WARNING: MATRIX MATH AHEAD, PROCEED WITH CAUTION)**. This is deeper than like **CS5131** so you will learn some stuff with this excercise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd43e5d",
   "metadata": {},
   "source": [
    "This article is written with some assumed knowledge of the reader but it is not that bad for most CS students especially since NNs are baby level for the most part. Nonetheless, assumed knowledge is written below.\n",
    "- Deep Neural Network (How to implement + basic understanding of the math)\n",
    "- Gradient Descent\n",
    "- Linear Algebra (vomit)\n",
    "- Convolution\n",
    "- Max Pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020e3966",
   "metadata": {},
   "source": [
    "This is the first part of like a multipart (likely 2) series about Math behind deep and convolutional neural networks. This article is about the proper in-depth math and implementation, to the correee."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2adfb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4ac74c",
   "metadata": {},
   "source": [
    "# Gradient Descent Example (Linear System Solution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec85bc2",
   "metadata": {},
   "source": [
    "Observe the following series of mathematical equations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5abbe3c",
   "metadata": {},
   "source": [
    "$$\n",
    "4a+2b=22\\\\\n",
    "3a+8b=49\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf7f851",
   "metadata": {},
   "source": [
    "I literally forgot how to solve these so I will use gradient descent to solve them"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c17859",
   "metadata": {},
   "source": [
    "If you remember, Gradient Descent is a method used to solve any sort of equation by taking steps towards the real value by using calculus to predict the direction and size of the step. Essentially if you remember in calculus, the minimum of the graph will have a tangent of slope 0 and hence we can understand the direction of these \"steps\" to solve the problem. We just need a function where the derivative and function result approach 0 as you get closer to the true solution. This function is known as the objective function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bdbaa2e",
   "metadata": {},
   "source": [
    "As you probably know, a linear equation is written as such"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd3b832",
   "metadata": {},
   "source": [
    "$$\n",
    "A \\mathbf{x}-\\mathbf{b}=0\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f114d6a",
   "metadata": {},
   "source": [
    "where $A$ is a known square matrix, $\\mathbf{b}$ is a known vector and $\\mathbf{x}$ is an unknown vector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c168af7",
   "metadata": {},
   "source": [
    "In this case, for the objective function we will use linear least square functions as it is an accurate thing to minimize in this case written below. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb6073d",
   "metadata": {},
   "source": [
    "$$F(\\mathbf{x}) = {||A\\mathbf{x}-\\mathbf{b}||}_{2}^{2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13002962",
   "metadata": {},
   "source": [
    "## Matrix Calculus speedbump"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a6367b",
   "metadata": {},
   "source": [
    "Now, what the frick does the weird lines and the 2 2's above mean and how the heck do we calculate the derivative of a scalar in terms of a vector (that sounds very funny). Well we have to learn matrix calculus, a very cringe domain of math that is very cringe. Ideally, you want to avoid this at all cost, but I will do a gentle walk through this stuff."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c248f1dd",
   "metadata": {},
   "source": [
    "First lets remember standard derivatives with a simple example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5284576",
   "metadata": {},
   "source": [
    "$$\n",
    "y=sin(x^2)+5\\\\\n",
    "\\frac{dy}{dx}=\\frac{d}{dx}(sin(x^2)+5)\\\\\n",
    "=2xcos(x^2)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a712a50",
   "metadata": {},
   "source": [
    "For functions with multiple variables, we can find the partial derivative to each of the vaiables. Shown in the simple example below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae69d76",
   "metadata": {},
   "source": [
    "$$f(x,y)=3xy+x^2\\\\\n",
    "\\frac{\\partial f(x,y)}{\\partial x}=3y+2x\\\\\n",
    "\\frac{\\partial f(x,y)}{\\partial y}=3x\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2364ab08",
   "metadata": {},
   "source": [
    "A thing to understand is that vectors are just a collection of numbers which means its just a multivariable function, so if the function is $f:\\mathbb{R}^{n} \\rightarrow \\mathbb{R}$ (the derivative is known as the gradient) We need to find the output with respect to every value in the vector. Hence, there are n partial derivatives. But do we represent these n partial derivatives as a column vector or row vector?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76470a31",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial y}{\\partial\\mathbf{x}} = \n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial y}{\\partial{\\mathbf{x}}_{1}}\\\\\n",
    "\\frac{\\partial y}{\\partial{\\mathbf{x}}_{2}}\\\\\n",
    "\\cdots\\\\\n",
    "\\frac{\\partial y}{\\partial{\\mathbf{x}}_{n}}\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial y}{\\partial\\mathbf{x}} = \n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial y}{\\partial{\\mathbf{x}}_{1}} & \\frac{\\partial y}{\\partial{\\mathbf{x}}_{2}} & \\cdots & \\frac{\\partial y}{\\partial{\\mathbf{x}}_{n}}\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7bb7fad",
   "metadata": {},
   "source": [
    "Well, both actually can work (even if you think of a vector as a column vector), the first version is called the denominator layout and the second one is called the numerator layout. They are both transpositions of each other. For gradient descent the denominator layout is more natural because for standard practice because we think of a vector as a column vector. I also prefer the denominator layout. However, the numerator layout follows the rules of single variable calculus more normally and will be much easier to follow. For example, matrices do not have commutative multiplication so the direction you chain terms matters. We naturally think of chaining terms to the back and this is true for numerator layout but in denominator layout terms are chained to the front. Product rule also is more funny when it comes to denom layout. So moving forward we will stick with the numerator layout and transpose the matrix or vector once the derivative is found. We will also stick to column vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1eaa3c",
   "metadata": {},
   "source": [
    "First lets look at the $A\\mathbf{x}-\\mathbf{b}$ term and we will see why the derivative is so and so with a simple $2 \\times 2$ case. $A\\mathbf{x}-\\mathbf{b}$ is a $f:\\mathbb{R}^{n} \\rightarrow \\mathbb{R}^{n}$ and hence the derivative will be a matrix (known as the jacobian to many). Lets first, see the general equation and work it out for every value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7d8e3c",
   "metadata": {},
   "source": [
    "$$\\mathbf{y} = A\\mathbf{x}-\\mathbf{b}\\\\\n",
    "\\begin{bmatrix}\n",
    "{\\mathbf{y}}_{1} \\\\\n",
    "{\\mathbf{y}}_{2}\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "{a}_{11} & {a}_{12}\\\\\n",
    "{a}_{21} & {a}_{22}\\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "{\\mathbf{x}}_{1} \\\\\n",
    "{\\mathbf{x}}_{2}\n",
    "\\end{bmatrix}\n",
    "-\n",
    "\\begin{bmatrix}\n",
    "{\\mathbf{b}}_{1} \\\\\n",
    "{\\mathbf{b}}_{2}\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "{a}_{11}{\\mathbf{x}}_{1} + {a}_{12}{\\mathbf{x}}_{2}-{\\mathbf{b}}_{1} \\\\\n",
    "{a}_{21}{\\mathbf{x}}_{1} + {a}_{22}{\\mathbf{x}}_{2}-{\\mathbf{b}}_{1}\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731b0257",
   "metadata": {},
   "source": [
    "Now we calculate the Jacobian (remember it is transposed) by calculating the individual derivative for every value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02cdb4f0",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}} =\n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial {\\mathbf{y}}_{1}}{\\partial{\\mathbf{x}}_{1}} & \\frac{\\partial {\\mathbf{y}}_{1}}{\\partial{\\mathbf{x}}_{2}}\\\\\n",
    "\\frac{\\partial {\\mathbf{y}}_{2}}{\\partial{\\mathbf{x}}_{1}} & \\frac{\\partial {\\mathbf{y}}_{2}}{\\partial{\\mathbf{x}}_{2}}\\\\\n",
    "\\end{bmatrix}\\\\\n",
    "\\frac{\\partial {\\mathbf{y}}_{1}}{\\partial{\\mathbf{x}}_{1}}={a}_{11}\\\\\n",
    "\\frac{\\partial {\\mathbf{y}}_{1}}{\\partial{\\mathbf{x}}_{2}}={a}_{12}\\\\\n",
    "\\frac{\\partial {\\mathbf{y}}_{2}}{\\partial{\\mathbf{x}}_{1}}={a}_{21}\\\\\n",
    "\\frac{\\partial {\\mathbf{y}}_{2}}{\\partial{\\mathbf{x}}_{2}}={a}_{22}\\\\\n",
    "\\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}} =\n",
    "\\begin{bmatrix}\n",
    "{a}_{11} & {a}_{12}\\\\\n",
    "{a}_{21} & {a}_{22}\\\\\n",
    "\\end{bmatrix}\n",
    "=A\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a4ab0a",
   "metadata": {},
   "source": [
    "We see that it is kind of the same with single variable, where if we have $f(x)=ax$, then $f'(x)=a$ where a is constant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c4d791",
   "metadata": {},
   "source": [
    "Now we look at the funny lines and twos. This is a common function known as the euclidean norm or 2-norm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a771599",
   "metadata": {},
   "source": [
    "$$\\|{\\mathbf {x}}\\|_{2}:={\\sqrt {x_{1}^{2}+\\cdots +x_{n}^{2}}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7175f912",
   "metadata": {},
   "source": [
    "We then square it giving rise to the second 2. Now we define and do the same thing we did with $Ax-b$, $\\|{\\mathbf {y}}\\|_{2}^{2}$ is $f:\\mathbb{R}^{n} \\rightarrow \\mathbb{R}$. Hence, the derivative is a row vector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8ca7e9",
   "metadata": {},
   "source": [
    "$$z=\\|{\\mathbf {y}}\\|_{2}^{2}\\\\\n",
    "z={\\mathbf {y}}_{1}^{2} + {\\mathbf {y}}_{2}^{2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd2b989",
   "metadata": {},
   "source": [
    "Now we calculate the Gradient (remember it is transposed) by calculating the individual derivative for every value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2befec21",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial F(\\mathbf{x})}{\\partial\\mathbf{y}} = \n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial F(\\mathbf{x})}{\\partial{\\mathbf{y}}_{1}} & \\frac{\\partial F(\\mathbf{x})}{\\partial{\\mathbf{y}}_{2}}\n",
    "\\end{bmatrix}\\\\\n",
    "\\frac{\\partial F(\\mathbf{x})}{\\partial{\\mathbf{y}}_{1}}=2\\mathbf{y}_{1}\\\\\n",
    "\\frac{\\partial F(\\mathbf{x})}{\\partial{\\mathbf{y}}_{2}}=2\\mathbf{y}_{2}\\\\\n",
    "\\frac{\\partial F(\\mathbf{x})}{\\partial\\mathbf{y}} = \n",
    "\\begin{bmatrix}\n",
    "2\\mathbf{y}_{1} & 2\\mathbf{y}_{2}\n",
    "\\end{bmatrix}\n",
    "=2\\mathbf{y}^{T}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4131e2a",
   "metadata": {},
   "source": [
    "To illustrate the chain rule, I will calculate it individually and put it all together"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac0bf7d",
   "metadata": {},
   "source": [
    "$$F(\\mathbf{x}) = {||A\\mathbf{x}-\\mathbf{b}||}_{2}^{2}\\\\\n",
    "F(\\mathbf{x}) = {({a}_{11}{\\mathbf{x}}_{1} + {a}_{12}{\\mathbf{x}}_{2}-{\\mathbf{b}}_{1})}^{2} +\n",
    "{({a}_{21}{\\mathbf{x}}_{1} + {a}_{22}{\\mathbf{x}}_{2}-{\\mathbf{b}}_{1})}^{2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d63041",
   "metadata": {},
   "source": [
    "Now we calculate the Final Gradient by calculating the individual derivative for every value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2c9590",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial F(\\mathbf{x})}{\\partial\\mathbf{x}} = \n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial F(\\mathbf{x})}{\\partial{\\mathbf{x}}_{1}} & \\frac{\\partial F(\\mathbf{x})}{\\partial{\\mathbf{x}}_{2}}\n",
    "\\end{bmatrix}\\\\\n",
    "\\frac{\\partial F(\\mathbf{x})}{\\partial{\\mathbf{x}}_{1}}=2{a}_{11}({a}_{11}{\\mathbf{x}}_{1} + {a}_{12}{\\mathbf{x}}_{2}-{\\mathbf{b}}_{1}) + 2{a}_{21}({a}_{21}{\\mathbf{x}}_{1} + {a}_{22}{\\mathbf{x}}_{2}-{\\mathbf{b}}_{1})\\\\\n",
    "\\frac{\\partial F(\\mathbf{x})}{\\partial{\\mathbf{x}}_{2}}=2{a}_{12}({a}_{11}{\\mathbf{x}}_{1} + {a}_{12}{\\mathbf{x}}_{2}-{\\mathbf{b}}_{1}) + 2{a}_{22}({a}_{21}{\\mathbf{x}}_{1} + {a}_{22}{\\mathbf{x}}_{2}-{\\mathbf{b}}_{1})\\\\\n",
    "\\frac{\\partial F(\\mathbf{x})}{\\partial\\mathbf{x}} = \n",
    "\\begin{bmatrix}\n",
    "2{a}_{11}({a}_{11}{\\mathbf{x}}_{1} + {a}_{12}{\\mathbf{x}}_{2}-{\\mathbf{b}}_{1}) + 2{a}_{21}({a}_{21}{\\mathbf{x}}_{1} + {a}_{22}{\\mathbf{x}}_{2}-{\\mathbf{b}}_{1}) & 2{a}_{12}({a}_{11}{\\mathbf{x}}_{1} + {a}_{12}{\\mathbf{x}}_{2}-{\\mathbf{b}}_{1}) + 2{a}_{22}({a}_{21}{\\mathbf{x}}_{1} + {a}_{22}{\\mathbf{x}}_{2}-{\\mathbf{b}}_{1})\n",
    "\\end{bmatrix}\\\\\n",
    "=2\n",
    "\\begin{bmatrix}\n",
    "{a}_{11}{\\mathbf{x}}_{1} + {a}_{12}{\\mathbf{x}}_{2}-{\\mathbf{b}}_{1} &\n",
    "{a}_{21}{\\mathbf{x}}_{1} + {a}_{22}{\\mathbf{x}}_{2}-{\\mathbf{b}}_{1}\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "{a}_{11} & {a}_{12}\\\\\n",
    "{a}_{21} & {a}_{22}\\\\\n",
    "\\end{bmatrix}\n",
    "=2{(A\\mathbf{x}-\\mathbf{b})}^{T}A\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed72055",
   "metadata": {},
   "source": [
    "As we can see from that last step, its pretty gory, but you can see how neat matrix notation is as compared to writing all that out and you see how matrix calc works. With numerator layout, its very similar to single var but with like a few extra steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0427a249",
   "metadata": {},
   "source": [
    "I then transpose the derivative back into the denominator layout written below. The step function is also written below which we will use for the gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c578aff4",
   "metadata": {},
   "source": [
    "$$F(\\mathbf{x}) = {||A\\mathbf{x}-\\mathbf{b}||}^{2}\\\\\n",
    "\\nabla F(\\mathbf {x} )=2A^{T}(A\\mathbf {x} -\\mathbf {b})\\\\\n",
    "\\mathbf {x}_{n+1}=\\mathbf {x}_{n}-\\gamma \\nabla F(\\mathbf {x} _{n})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb5bef0",
   "metadata": {},
   "source": [
    "where $\\gamma$ is the learning rate, we need a small learning rate as it prevents the function from taking large steps and objective functions tend to overblow the \"true\" error of a function. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e3807a",
   "metadata": {},
   "source": [
    "We can now implement this in code form for a very simple linear system written below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f422b97",
   "metadata": {},
   "source": [
    "$$w+3x+2y-z=9\\\\\n",
    "5w+2x+y-2z=4\\\\\n",
    "x+2y+4z=24\\\\\n",
    "w+x-y-3z=-12$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8af483",
   "metadata": {},
   "source": [
    "written  as such in matrix form"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e59abc",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{bmatrix}\n",
    "1 & 3 & 2 & -1\\\\\n",
    "5 & 2 & 1 & -2\\\\\n",
    "0 & 1 & 2 & 4\\\\\n",
    "1 & 1 & -1 & -3\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "w\\\\\n",
    "x\\\\\n",
    "y\\\\\n",
    "z\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "9\\\\\n",
    "4\\\\\n",
    "24\\\\\n",
    "-12\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d609a889",
   "metadata": {},
   "source": [
    "This is the math and the respective python code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b43a99a",
   "metadata": {},
   "source": [
    "$$\n",
    "A=\n",
    "\\begin{bmatrix}\n",
    "1 & 3 & 2 & -1\\\\\n",
    "5 & 2 & 1 & -2\\\\\n",
    "0 & 1 & 2 & 4\\\\\n",
    "1 & 1 & -1 & -3\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "185f238f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  3.,  2., -1.],\n",
       "       [ 5.,  2.,  1., -2.],\n",
       "       [ 0.,  1.,  2.,  4.],\n",
       "       [ 1.,  1., -1., -3.]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.array([[1,3,2,-1],[5,2,1,-2],[0,1,2,4],[1,1,-1,-3]], dtype=np.float64)\n",
    "A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2a7ec2",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{b}=\n",
    "\\begin{bmatrix}\n",
    "9\\\\\n",
    "4\\\\\n",
    "24\\\\\n",
    "-12\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc9532d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  9.],\n",
       "       [  4.],\n",
       "       [ 24.],\n",
       "       [-12.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = np.array([[9],[4],[24],[-12]], dtype=np.float64)\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3045b63",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{x}=\n",
    "\\begin{bmatrix}\n",
    "w\\\\\n",
    "x\\\\\n",
    "y\\\\\n",
    "z\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a72db4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.04582682],\n",
       "       [0.11868215],\n",
       "       [0.97772753],\n",
       "       [0.66042745]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.random.rand(4,1)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3272077b",
   "metadata": {},
   "source": [
    "$$F(\\mathbf{x}) = {||A\\mathbf{x}-\\mathbf{b}||}^{2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55e3e7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_function(x):\n",
    "    return np.linalg.norm(np.matmul(A,x) - b)**2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914ff6ba",
   "metadata": {},
   "source": [
    "$$\\nabla F(\\mathbf {x} )=2A^{T}(A\\mathbf {x} -\\mathbf {b})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6ba1e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_function_derivative(x):\n",
    "    return 2 * np.matmul(A.T, np.matmul(A,x) - b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18235d73",
   "metadata": {},
   "source": [
    "In this case, I implemented an arbritary learning rate and arbritary step count. In traditional non-machine learning gradient descent, the learning rate changes per step and is determined via a heuristic such as the Barzilai–Borwein method, however this is not necessary as gradient descent is very robust. I used an arbritary step count for simplicity but you should ideally use some sort of boolean condition to break the loop such as $F(\\mathbf{x})<0.01$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b466c6",
   "metadata": {},
   "source": [
    "$$\\mathbf {x}_{n+1}=\\mathbf {x}_{n}-\\gamma \\nabla F(\\mathbf {x} _{n})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "489d4f37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.],\n",
       "       [2.],\n",
       "       [3.],\n",
       "       [4.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learning_rate = 0.01\n",
    "for i in range(5000):\n",
    "    x -= learning_rate * objective_function_derivative(x)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ecd0795b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  9.],\n",
       "       [  4.],\n",
       "       [ 24.],\n",
       "       [-12.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.matmul(A,x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efce145",
   "metadata": {},
   "source": [
    "Voila, we have solved the equation with gradient descent, and the solution is super close. This shows the power of gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0803710",
   "metadata": {},
   "source": [
    "# Deep Neural Network Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d15e024",
   "metadata": {},
   "source": [
    "To understand the math behind it, we will first look at the single perceptron case"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1063da0",
   "metadata": {},
   "source": [
    "<img src=\"images/single_perceptron_example.png\" width=500/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d875a4db",
   "metadata": {},
   "source": [
    "$$z=xw+b\\\\\n",
    "a=\\sigma (z)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9314c69",
   "metadata": {},
   "source": [
    "where $w$ is the weight, $b$ is the bias, $x$ is the input, $\\sigma$ is the activation function and $a$ is the output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee449725",
   "metadata": {},
   "source": [
    "We assume that this is a single layer network and that the loss function is just applied after, and we will just use the MSE loss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb067a15",
   "metadata": {},
   "source": [
    "$$L = {(a-y)}^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b07a8a5",
   "metadata": {},
   "source": [
    "where $y$ is the true y, $L$ is the cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a25ce49",
   "metadata": {},
   "source": [
    "In this case, it is quite easy to represent. Let us buff it up to a layer with 4 input neurons and 4 output neurons for example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8463cad5",
   "metadata": {},
   "source": [
    "<img src=\"images/multiple_perceptron_example.png\" width=500/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462bde60",
   "metadata": {},
   "source": [
    "$${w}_{11}{x}_{1} + {w}_{21}{x}_{2} + {w}_{31}{x}_{3} + {w}_{41}{x}_{4} + {b}_{1}={z}_{1}\\\\\n",
    "{w}_{12}{x}_{1} + {w}_{22}{x}_{2} + {w}_{32}{x}_{3} + {w}_{42}{x}_{4} + {b}_{2}={z}_{2}\\\\\n",
    "{w}_{13}{x}_{1} + {w}_{23}{x}_{2} + {w}_{33}{x}_{3} + {w}_{43}{x}_{4} + {b}_{3}={z}_{3}\\\\\n",
    "{w}_{14}{x}_{1} + {w}_{24}{x}_{2} + {w}_{34}{x}_{3} + {w}_{44}{x}_{4} + {b}_{4}={z}_{4}\\\\\n",
    "{a}_{1}=\\sigma({z}_{1})\\quad\n",
    "{a}_{2}=\\sigma({z}_{2})\\quad\n",
    "{a}_{3}=\\sigma({z}_{3})\\quad\n",
    "{a}_{4}=\\sigma({z}_{4})\\\\\n",
    "c=\\frac{1}{4}({({a}_{1}-{y}_{1})}^{2} + {({a}_{2}-{y}_{2})}^{2} + {({a}_{3}-{y}_{3})}^{2} + {({a}_{4}-{y}_{4})}^{2})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a47e067",
   "metadata": {},
   "source": [
    "As you can see, this is just a linear system much like the one showed in the example and it becomes very simple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e3e683",
   "metadata": {},
   "source": [
    "$$\\mathbf{z} = W\\mathbf{x} + \\mathbf{b}\\\\\n",
    "\\mathbf{a} = \\sigma(\\mathbf{z})\\\\\n",
    "c=\\frac{1}{n}\\|{\\mathbf{a}-\\mathbf {y}}\\|_{2}^{2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1bce00",
   "metadata": {},
   "source": [
    "We know from our work earlier we know that"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a74eeb2",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial \\mathbf{z}}{\\partial \\mathbf{b}}=I\\\\\n",
    "\\frac{\\partial \\mathbf{z}}{\\partial \\mathbf{x}}=W\\\\\n",
    "\\frac{\\partial c}{\\partial \\mathbf{a}}=\\frac{2}{n}{(\\mathbf{a}-\\mathbf{y})}^{T}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a727b38d",
   "metadata": {},
   "source": [
    "However we have once again hit into a funny speedbump, how do we find the derivative of a vector $\\mathbf{z}$ with respect to a matrix $W$. The function is of the form $f:\\mathbb{R}^{m \\times n} \\rightarrow \\mathbb{R}^{m}$. Hence, the derivative will be a third order tensor but for now we will use a trick to dodge the usage of third order tensors because of the nature of the function $W\\mathbf{x}$. For this example, I use $m=3$ and $n=2$ but its generalizable for any sizes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6d62c3",
   "metadata": {},
   "source": [
    "$$\\mathbf{z} = W\\mathbf{x} + \\mathbf{b}\\\\\n",
    "\\begin{bmatrix}\n",
    "{\\mathbf{z}}_{1} \\\\\n",
    "{\\mathbf{z}}_{2} \\\\\n",
    "{\\mathbf{z}}_{3}\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "{w}_{11} & {w}_{12}\\\\\n",
    "{w}_{21} & {w}_{22}\\\\\n",
    "{w}_{31} & {w}_{32}\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "{\\mathbf{x}}_{1} \\\\\n",
    "{\\mathbf{x}}_{2}\n",
    "\\end{bmatrix}\n",
    "+\n",
    "\\begin{bmatrix}\n",
    "{\\mathbf{b}}_{1} \\\\\n",
    "{\\mathbf{b}}_{2} \\\\\n",
    "{\\mathbf{b}}_{3}\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "{w}_{11}{\\mathbf{x}}_{1} + {w}_{12}{\\mathbf{x}}_{2} + {\\mathbf{b}}_{1}\\\\\n",
    "{w}_{21}{\\mathbf{x}}_{1} + {w}_{22}{\\mathbf{x}}_{2} + {\\mathbf{b}}_{1}\\\\\n",
    "{w}_{31}{\\mathbf{x}}_{1} + {w}_{32}{\\mathbf{x}}_{2} + {\\mathbf{b}}_{1}\\\\\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f8ed75",
   "metadata": {},
   "source": [
    "We now calculate the individual derivatives of $\\mathbf{z}$ wrt to $W$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5bbee4",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial \\mathbf{z}_{1}}{\\partial w_{11}}=\\mathbf{x}_{1}\\quad\n",
    "\\frac{\\partial \\mathbf{z}_{2}}{\\partial w_{11}}=0\\quad\n",
    "\\frac{\\partial \\mathbf{z}_{3}}{\\partial w_{11}}=0\\\\\n",
    "\\frac{\\partial \\mathbf{z}_{1}}{\\partial w_{12}}=\\mathbf{x}_{2}\\quad\n",
    "\\frac{\\partial \\mathbf{z}_{2}}{\\partial w_{12}}=0\\quad\n",
    "\\frac{\\partial \\mathbf{z}_{3}}{\\partial w_{12}}=0\\\\\n",
    "\\frac{\\partial \\mathbf{z}_{1}}{\\partial w_{21}}=0\\quad\n",
    "\\frac{\\partial \\mathbf{z}_{2}}{\\partial w_{21}}=\\mathbf{x}_{1}\\quad\n",
    "\\frac{\\partial \\mathbf{z}_{3}}{\\partial w_{21}}=0\\\\\n",
    "\\frac{\\partial \\mathbf{z}_{1}}{\\partial w_{22}}=0\\quad\n",
    "\\frac{\\partial \\mathbf{z}_{2}}{\\partial w_{22}}=\\mathbf{x}_{2}\\quad\n",
    "\\frac{\\partial \\mathbf{z}_{3}}{\\partial w_{22}}=0\\\\\n",
    "\\frac{\\partial \\mathbf{z}_{1}}{\\partial w_{31}}=0\\quad\n",
    "\\frac{\\partial \\mathbf{z}_{2}}{\\partial w_{31}}=0\\quad\n",
    "\\frac{\\partial \\mathbf{z}_{3}}{\\partial w_{31}}=\\mathbf{x}_{1}\\\\\n",
    "\\frac{\\partial \\mathbf{z}_{1}}{\\partial w_{32}}=0\\quad\n",
    "\\frac{\\partial \\mathbf{z}_{2}}{\\partial w_{32}}=0\\quad\n",
    "\\frac{\\partial \\mathbf{z}_{3}}{\\partial w_{32}}=\\mathbf{x}_{2}\\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2e2791",
   "metadata": {},
   "source": [
    "We see that this is a pretty complex looking tensor but we see that a majority of the values are 0 allowing us to pull of an epic hack by considering the fact that at the end we are essentially trying to get a singular scalar value (the loss) and find the partial derivative of that wrt to $W$. There are some steps involved in getting from $\\mathbf{z}$ to $c$ but for simplicity instead of showing everything, we will condense all of this into a function $f:\\mathbb{R}^{n} \\rightarrow \\mathbb{R}$ which is defined as $c=f(\\mathbf{z})$. In this case, we know the tensor values and we know the gradient and what the derivative should be. Hence, we now just evaluate it and see if we can see any property"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f155093",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial c}{\\partial\\mathbf{z}} = \n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial c}{\\partial{\\mathbf{z}}_{1}} & \\frac{\\partial c}{\\partial{\\mathbf{z}}_{2}} & \\frac{\\partial c}{\\partial{\\mathbf{z}}_{2}}\n",
    "\\end{bmatrix}\\\\\n",
    "\\frac{\\partial c}{\\partial W} = \n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial c}{\\partial{w}_{11}} & \\frac{\\partial c}{\\partial{w}_{21}} & \\frac{\\partial c}{\\partial{w}_{31}}\\\\\n",
    "\\frac{\\partial c}{\\partial{w}_{12}} & \\frac{\\partial c}{\\partial{w}_{22}} & \\frac{\\partial c}{\\partial{w}_{32}}\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\frac{\\partial c}{\\partial \\mathbf{z}}\\frac{\\partial \\mathbf{z}}{\\partial \\mathbf{W}}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial c}{\\partial{\\mathbf{z}}_{1}}\\frac{\\partial {\\mathbf{z}}_{1}}{\\partial{w}_{11}} & \\frac{\\partial c}{\\partial{\\mathbf{z}}_{2}}\\frac{\\partial {\\mathbf{z}}_{2}}{\\partial{w}_{21}} & \\frac{\\partial c}{\\partial{\\mathbf{z}}_{3}}\\frac{\\partial {\\mathbf{z}}_{3}}{\\partial{w}_{31}}\\\\\n",
    "\\frac{\\partial c}{\\partial{\\mathbf{z}}_{1}}\\frac{\\partial {\\mathbf{z}}_{1}}{\\partial{w}_{12}} & \\frac{\\partial c}{\\partial{\\mathbf{z}}_{2}}\\frac{\\partial {\\mathbf{z}}_{2}}{\\partial{w}_{22}} & \\frac{\\partial c}{\\partial{\\mathbf{z}}_{3}}\\frac{\\partial {\\mathbf{z}}_{3}}{\\partial{w}_{32}}\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial c}{\\partial{\\mathbf{z}}_{1}}\\mathbf{x}_{1} & \\frac{\\partial c}{\\partial{\\mathbf{z}}_{2}}\\mathbf{x}_{1} & \\frac{\\partial c}{\\partial{\\mathbf{z}}_{3}}\\mathbf{x}_{1}\\\\\n",
    "\\frac{\\partial c}{\\partial{\\mathbf{z}}_{1}}\\mathbf{x}_{2} & \\frac{\\partial c}{\\partial{\\mathbf{z}}_{2}}\\mathbf{x}_{2} & \\frac{\\partial c}{\\partial{\\mathbf{z}}_{3}}\\mathbf{x}_{2}\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\mathbf{x}\\frac{\\partial c}{\\partial\\mathbf{z}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38b6e66",
   "metadata": {},
   "source": [
    "Wonderful, we have just found out this epic hack, where we just add $\\mathbf{x}$ to the front. Normally this kind of hack is not possible but it is just possible in this special case as we dont have to consider terms such as $\\frac{\\partial c}{\\partial{\\mathbf{z}}_{2}}\\frac{\\partial {\\mathbf{z}}_{2}}{\\partial{w}_{11}}$ because they are just 0. It helps us dodge all the possibilites of tensor calculus (FOR NOW) and allows the numpy multiplication to be much easier. $f$ can also generalize for any vector to scalar function, not just the specific steps we make."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4772cf",
   "metadata": {},
   "source": [
    "The next speedbump is much more easier to grasp then the last one, and that is element wise operations. In this case, we have the activation function $\\sigma:\\mathbb{R}^{n} \\rightarrow \\mathbb{R}^{n}$ or $\\sigma:\\mathbb{R} \\rightarrow \\mathbb{R}$ which looks like a sigmoid, but this is just a placeholder, it can be any real to real activation function, such as relu or whatever the cool kids use these days. Once again, we work it out for every value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50890d25",
   "metadata": {},
   "source": [
    "$$\\mathbf{a} = \\sigma(\\mathbf{z})\\\\\n",
    "\\begin{bmatrix}\n",
    "{\\mathbf{a}}_{1} \\\\\n",
    "{\\mathbf{a}}_{2} \\\\\n",
    "{\\mathbf{a}}_{3}\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\sigma(\n",
    "\\begin{bmatrix}\n",
    "{\\mathbf{z}}_{1} \\\\\n",
    "{\\mathbf{z}}_{2} \\\\\n",
    "{\\mathbf{z}}_{3}\n",
    "\\end{bmatrix})\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "\\sigma({\\mathbf{z}}_{1}) \\\\\n",
    "\\sigma({\\mathbf{z}}_{2}) \\\\\n",
    "\\sigma({\\mathbf{z}}_{3})\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06d8646",
   "metadata": {},
   "source": [
    "Now for the 48th billion time, we calculate the jacobian for every individual derivative to get the general property of the operation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0182e90",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial \\mathbf{a}}{\\partial \\mathbf{z}} =\n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial {\\mathbf{a}}_{1}}{\\partial{\\mathbf{z}}_{1}} & \\frac{\\partial {\\mathbf{a}}_{1}}{\\partial{\\mathbf{z}}_{2}}& \\frac{\\partial {\\mathbf{a}}_{1}}{\\partial{\\mathbf{z}}_{3}}\\\\\n",
    "\\frac{\\partial {\\mathbf{a}}_{2}}{\\partial{\\mathbf{z}}_{1}} & \\frac{\\partial {\\mathbf{a}}_{2}}{\\partial{\\mathbf{z}}_{2}} & \\frac{\\partial {\\mathbf{a}}_{2}}{\\partial{\\mathbf{z}}_{3}}\\\\\n",
    "\\frac{\\partial {\\mathbf{a}}_{3}}{\\partial{\\mathbf{z}}_{1}} & \\frac{\\partial {\\mathbf{a}}_{3}}{\\partial{\\mathbf{z}}_{2}} & \\frac{\\partial {\\mathbf{a}}_{3}}{\\partial{\\mathbf{z}}_{3}}\n",
    "\\end{bmatrix}\\\\\n",
    "\\frac{\\partial {\\mathbf{a}}_{1}}{\\partial{\\mathbf{z}}_{1}}=\\sigma^{'}(\\mathbf{z}_{1})\\quad\n",
    "\\frac{\\partial {\\mathbf{a}}_{1}}{\\partial{\\mathbf{z}}_{2}}=0\\quad\n",
    "\\frac{\\partial {\\mathbf{a}}_{1}}{\\partial{\\mathbf{z}}_{3}}=0\\\\\n",
    "\\frac{\\partial {\\mathbf{a}}_{2}}{\\partial{\\mathbf{z}}_{1}}=0\\quad\n",
    "\\frac{\\partial {\\mathbf{a}}_{2}}{\\partial{\\mathbf{z}}_{2}}=\\sigma^{'}(\\mathbf{z}_{2})\\quad\n",
    "\\frac{\\partial {\\mathbf{a}}_{2}}{\\partial{\\mathbf{z}}_{3}}=0\\\\\n",
    "\\frac{\\partial {\\mathbf{a}}_{3}}{\\partial{\\mathbf{z}}_{1}}=0\\quad\n",
    "\\frac{\\partial {\\mathbf{a}}_{3}}{\\partial{\\mathbf{z}}_{2}}=0\\quad\n",
    "\\frac{\\partial {\\mathbf{a}}_{3}}{\\partial{\\mathbf{z}}_{3}}=\\sigma^{'}(\\mathbf{z}_{3})\\\\\n",
    "\\frac{\\partial \\mathbf{a}}{\\partial \\mathbf{z}} =\n",
    "\\begin{bmatrix}\n",
    "\\sigma^{'}(\\mathbf{z}_{1}) & 0 & 0\\\\\n",
    "0 & \\sigma^{'}(\\mathbf{z}_{2}) & 0\\\\\n",
    "0 & 0 & \\sigma^{'}(\\mathbf{z}_{3})\\\\\n",
    "\\end{bmatrix}\n",
    "=diag(\\sigma^{'}(\\mathbf{z}))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0af6460",
   "metadata": {},
   "source": [
    "As you see, we can reduce this derivative to this value. I used the $diag$ operator which converts a vector to a diagonal matrix "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465a76ae",
   "metadata": {},
   "source": [
    "Finally, after all this derivation (literally and figuratively) we can chain rule everything together"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e41885",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial c}{\\partial \\mathbf{b}}=\\frac{\\partial c}{\\partial \\mathbf{a}}\\frac{\\partial \\mathbf{a}}{\\partial \\mathbf{z}}\\frac{\\partial \\mathbf{z}}{\\partial \\mathbf{b}}\n",
    "=\n",
    "\\frac{2}{n}{(\\mathbf{a}-\\mathbf{y})}^{T}diag(\\sigma^{'}(\\mathbf{z}))\\\\\n",
    "\\frac{\\partial c}{\\partial \\mathbf{x}}=\\frac{\\partial c}{\\partial \\mathbf{a}}\\frac{\\partial \\mathbf{a}}{\\partial \\mathbf{z}}\\frac{\\partial \\mathbf{z}}{\\partial \\mathbf{x}}\n",
    "=\n",
    "\\frac{2}{n}{(\\mathbf{a}-\\mathbf{y})}^{T}diag(\\sigma^{'}(\\mathbf{z}))W\\\\\n",
    "\\frac{\\partial c}{\\partial W}=\\frac{\\partial c}{\\partial \\mathbf{a}}\\frac{\\partial \\mathbf{a}}{\\partial \\mathbf{z}}\\frac{\\partial \\mathbf{z}}{\\partial W}\n",
    "=\n",
    "\\frac{2}{n}\\mathbf{x}{(\\mathbf{a}-\\mathbf{y})}^{T}diag(\\sigma^{'}(\\mathbf{z}))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7db67f",
   "metadata": {},
   "source": [
    "Now that we got these simple definitions for the 1 layer case, we can expand it to the multi-layer case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6172afaf",
   "metadata": {},
   "source": [
    "$$\\mathbf{a}_{0}=\\mathbf{x}\\\\\n",
    "\\mathbf{z}_{i}={W}_{i-1}{\\mathbf{a}}_{i-1} + \\mathbf{b}_{i-1}\\\\\n",
    "\\mathbf{a}_{i}=\\sigma(\\mathbf{z}_{i})\\\\\n",
    "i = 1,2,3,...,L\\\\\n",
    "c=\\frac{1}{n}\\|{\\mathbf{a}-\\mathbf {y}}\\|_{2}^{2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a74b46",
   "metadata": {},
   "source": [
    "We can do the calculus for this now for bias and weight for the average layer using the chain rule"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c81b7ae",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial c}{\\partial \\mathbf{b}_{i-1}}=\\frac{\\partial c}{\\partial \\mathbf{a}_{L}}\\frac{\\partial \\mathbf{a}_L}{\\partial \\mathbf{z}_{L}}\\frac{\\partial \\mathbf{z}_{L}}{\\partial \\mathbf{a}_{L-1}}\\cdots\\frac{\\partial \\mathbf{a}_{i}}{\\partial \\mathbf{z}_{i}}\\frac{\\partial \\mathbf{z}_{i}}{\\partial \\mathbf{b}_{i-1}}=\n",
    "\\frac{2}{n}{(\\mathbf{a}_L-\\mathbf{y})}^{T}diag(\\sigma^{'}(\\mathbf{z}_L))W_{L-1}\\cdots diag(\\sigma^{'}(\\mathbf{z}_i))\\\\\n",
    "\\frac{\\partial c}{\\partial W_{i-1}}=\\frac{\\partial c}{\\partial \\mathbf{a}_{L}}\\frac{\\partial \\mathbf{a}_L}{\\partial \\mathbf{z}_{L}}\\frac{\\partial \\mathbf{z}_{L}}{\\partial \\mathbf{a}_{L-1}}\\cdots\\frac{\\partial \\mathbf{a}_{i}}{\\partial \\mathbf{z}_{i}}\\frac{\\partial \\mathbf{z}_{i}}{\\partial W_{i-1}}=\n",
    "\\frac{2}{n}\\mathbf{a}_{i-1}{(\\mathbf{a}_L-\\mathbf{y})}^{T}diag(\\sigma^{'}(\\mathbf{z}_L))W_{L-1}\\cdots diag(\\sigma^{'}(\\mathbf{z}_i))\\\\\n",
    "i = 1,2,3,...,L\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7660fd8c",
   "metadata": {},
   "source": [
    "Now it is time to actually implement this network FINALLY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15dddcee",
   "metadata": {},
   "source": [
    "# Neural Network Implementation (XNOR Gate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9103b96",
   "metadata": {},
   "source": [
    "I couldnt google like a good, very tiny dataset because everytime I searched I just found people complaining about there tiny datasets. So instead we are just gonna use our neural network to mimic the XNOR gate. Training, testing, whats that???!?!? But in all fairness, Im just trying to show you that the math works and how a neural network can mimic any function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970eb81a",
   "metadata": {},
   "source": [
    "<img src=\"images/XNOR_input_output.png\" width=180/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9f1880",
   "metadata": {},
   "source": [
    "For those who do not know, the XNOR gates inputs and outputs are written above. It is good for this example, because the inputs and outputs are all 0 and 1, it is fast to train and there is no bias in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56726212",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[np.array([[0],[0]], dtype=np.float64),np.array([[1]], dtype=np.float64)],\n",
    "        [np.array([[0],[1]], dtype=np.float64),np.array([[0]], dtype=np.float64)],\n",
    "        [np.array([[1],[0]], dtype=np.float64),np.array([[0]], dtype=np.float64)],\n",
    "        [np.array([[1],[1]], dtype=np.float64),np.array([[1]], dtype=np.float64)]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ecfa92",
   "metadata": {},
   "source": [
    "We then define, a network structure, it doesnt have to be to complex because it is such a simple function. I decided on a $2 \\rightarrow 3 \\rightarrow 1$ multi-layer perceptron structure, with sigmoid activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134bce6b",
   "metadata": {},
   "source": [
    "<img src=\"images/multiple_perceptron_network.png\" width=500/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eae4b6a",
   "metadata": {},
   "source": [
    "Next I code out all the math in this data class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9bb37dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNdata:\n",
    "    def __init__(self):\n",
    "        self.a_0 = None\n",
    "        self.W_0 = np.random.rand(3,2)\n",
    "        self.b_0 = np.random.rand(3,1)\n",
    "        self.z_1 = None\n",
    "        self.a_1 = None\n",
    "        self.W_1 = np.random.rand(1,3)\n",
    "        self.b_1 = np.random.rand(1,1)\n",
    "        self.z_2 = None\n",
    "        self.a_2 = None\n",
    "        self.db_1 = None\n",
    "        self.dw_1 = None\n",
    "        self.db_0 = None\n",
    "        self.dw_0 = None\n",
    "        \n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def sigmoid_derivative(self, x):\n",
    "        return self.sigmoid(x) * (1 - self.sigmoid(x))\n",
    "    \n",
    "    def feed_forward(self, x):\n",
    "        self.a_0 = x\n",
    "        \n",
    "        self.z_1 = np.matmul(self.W_0, self.a_0)+self.b_0\n",
    "        self.a_1 = self.sigmoid(self.z_1)\n",
    "        \n",
    "        self.z_2 = np.matmul(self.W_1, self.a_1)+self.b_1\n",
    "        self.a_2 = self.sigmoid(self.z_2)\n",
    "        return self.a_2\n",
    "        \n",
    "    def loss(self, y):\n",
    "        return np.linalg.norm(self.a_2-y)**2\n",
    "    \n",
    "    def back_prop(self, y):\n",
    "        dcdz_2 = 2 * np.matmul((self.a_2-y).T,np.diag(self.sigmoid_derivative(self.z_2).reshape(1)))\n",
    "        dcdb_1 = dcdz_2\n",
    "        dcdw_1 = np.matmul(self.a_1, dcdz_2)\n",
    "        \n",
    "        dcda_1 = np.matmul(dcdz_2, self.W_1)\n",
    "        dcdz_1 = np.matmul(dcda_1, np.diag(self.sigmoid_derivative(self.z_1).reshape(3)))\n",
    "        dcdb_0 = dcdz_1\n",
    "        dcdw_0 = np.matmul(self.a_0, dcdz_1)\n",
    "        \n",
    "        self.db_1 = dcdb_1.T\n",
    "        self.dw_1 = dcdw_1.T\n",
    "        self.db_0 = dcdb_0.T\n",
    "        self.dw_0 = dcdw_0.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf57e0f",
   "metadata": {},
   "source": [
    "Next I do gradient descent, there are 3 kinds of gradient descent when there are multiple datapoints, Stochastic, Batch and Minibatch. In SGD, the weights are updated after a single sample is run, this will obviously cause your step towards the ideal value be very chaotic. In batch GD, the weights are updated after every sample is run, and the net step is the sum/average of all the $\\nabla F(x)$, this is less chaotic, but steps are less frequent. Of course in real life, we can never know which algorithm is better without making an assumption about the data. (No Free Lunch Theorem) A good compromise is minibatch, which is like batch but use smaller chunks of all the datapoints every step. In this case, I use batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b2b9c08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss (1000/10000): 0.245\n",
      "loss (2000/10000): 0.186\n",
      "loss (3000/10000): 0.029\n",
      "loss (4000/10000): 0.007\n",
      "loss (5000/10000): 0.003\n",
      "loss (6000/10000): 0.002\n",
      "loss (7000/10000): 0.002\n",
      "loss (8000/10000): 0.001\n",
      "loss (9000/10000): 0.001\n",
      "loss (10000/10000): 0.001\n"
     ]
    }
   ],
   "source": [
    "nndata = NNdata()\n",
    "learning_rate = 0.1\n",
    "for i in range(10000):\n",
    "    db_1_batch = []\n",
    "    dw_1_batch = []\n",
    "    db_0_batch = []\n",
    "    dw_0_batch = []\n",
    "    c = []\n",
    "    for j in range(4):\n",
    "        nndata.feed_forward(data[j][0])\n",
    "        c.append(nndata.loss(data[j][1]))\n",
    "        nndata.back_prop(data[j][1])\n",
    "        db_1_batch.append(nndata.db_1)\n",
    "        dw_1_batch.append(nndata.dw_1)\n",
    "        db_0_batch.append(nndata.db_0)\n",
    "        dw_0_batch.append(nndata.dw_0)\n",
    "    if((i+1) % 1000 == 0):\n",
    "        print(\"loss (%d/10000): %.3f\" % (i+1, sum(c)/4))\n",
    "    nndata.b_1 -= learning_rate * sum(db_1_batch)\n",
    "    nndata.W_1 -= learning_rate * sum(dw_1_batch)\n",
    "    nndata.b_0 -= learning_rate * sum(db_0_batch)\n",
    "    nndata.W_0 -= learning_rate * sum(dw_0_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f6eb88",
   "metadata": {},
   "source": [
    "Viola, and we have officially done Neural networks from scratch. Pat yourself on the back for reading through this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff2725f",
   "metadata": {},
   "source": [
    "In the next part, it is time for the actual hard part. HAHAHHAHAHHAAH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa0689c",
   "metadata": {},
   "source": [
    "### Sources:\n",
    "A lot of people think I just collated a bunch of sources and rephrased, and honestly I walked into writing this thinking I would be doing just that. The problem is that many sources who have attempted to do this, only cover the single to multi perceptron layer case and not the multi to multi perceptron case. Which is pretty sad. The true math is hidden behind mountains of research papers that loosely connect to give the results of this blogpot which I cant figure out by myself. So, I did the math myself. Yes, it was a bit crazy, and it destroyed me to my core, but as my good friend Jude once said, I am slightly masochistic. This was a great character building moment for me. So these are the actual sources\n",
    "\n",
    "https://numpy.org/\n",
    "\n",
    "https://en.wikipedia.org/wiki/Gradient_descent\n",
    "\n",
    "https://en.wikipedia.org/wiki/Matrix_calculus\n",
    "\n",
    "https://en.wikipedia.org/wiki/Tensor_calculus\n",
    "\n",
    "https://en.wikipedia.org/wiki/Ricci_calculus\n",
    "\n",
    "https://en.wikipedia.org/wiki/XNOR_gate\n",
    "\n",
    "CS5131 notes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0b5794",
   "metadata": {},
   "source": [
    "All images are from the creative commons or are by me, if you wanna use any of the images I made or any of the math/code just credit me with like a \"(by Karimi Zayan)\" or something. Honestly, I can't check (CLOWN EMOJI) so uh it is your choice really."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
